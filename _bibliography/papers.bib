@inproceedings{10.1145/3746059.3747617,
author = {Wang, Xuetong and Pang, Ching Christie and Hui, Pan},
title = {Talking Spell: A Wearable System Enabling Real-Time Anthropomorphic Voice Interaction with Everyday Objects},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747617},
doi = {10.1145/3746059.3747617},
abstract = {Virtual assistants (VAs) have become ubiquitous in daily life, integrated into smartphones and smart devices, sparking interest in AI companions that enhance user experiences and foster emotional connections. However, existing companions are often embedded in specific objects—such as glasses, home assistants, or dolls—requiring users to form emotional bonds with unfamiliar items, which can lead to reduced engagement and feelings of detachment. To address this, we introduce Talking Spell1, a wearable system that empowers users to imbue any everyday object with speech and anthropomorphic personas through a user-centric radiative network. Leveraging advanced computer vision (e.g., YOLOv11[46] for object detection), large vision-language models (e.g., QWEN-VL[4] for persona generation), speech-to-text and text-to-speech technologies, Talking Spell guides users through three stages of emotional connection: acquaintance, familiarization, and bonding. We validated our system through a user study involving 12 participants, utilizing Talking Spell to explore four interaction intentions: entertainment, companionship, utility, and creativity. The results demonstrate its effectiveness in fostering meaningful interactions and emotional significance with everyday objects. Our findings indicate that Talking Spell creates engaging and personalized experiences, as demonstrated through various devices, ranging from accessories to essential wearables.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {117},
numpages = {17},
keywords = {Embodied and Explorable Interaction; AI Companionship; Wearable; On-body Devices; Ubiquitous Computing; Human-Object Interaction; Large Language Models (LLMs)},
location = {
},
selected={true},
preview={talkingspell.png},
series = {UIST '25}
}

@inproceedings{10.1145/3746058.3758985,
author = {Wang, Xuetong and Pang, Ching Christie and Hui, Pan},
title = {Demonstrating Talking Spell: A Wearable System for Real-Time Anthropomorphic Interaction with Everyday Objects},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758985},
doi = {10.1145/3746058.3758985},
abstract = {Virtual assistants, common in devices like smartphones, often struggle to form deep emotional connections due to their fixed forms. Talking Spell, a novel wearable system, addresses this by enabling users to endow everyday objects with speech and anthropomorphic personas, fostering engaging human-object interactions. Leveraging advanced computer vision (e.g., YOLOv11 for object detection), large vision-language models (e.g., QWEN-VL for persona generation), speech-to-text and text-to-speech technologies, Talking Spell guides users through emotional bonding stages, enhancing experiences in entertainment, companionship, utility, and creativity. We aim to advance AI companionship, with potential applications in education, mental health, and accessibility. Future developments will focus on integrating Talking Spell into smart environments and creating adaptive personas that evolve, deepening user connections.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {24},
numpages = {3},
keywords = {Embodied and Explorable Interaction; AI Companionship; Wearable; On-body Devices; Ubiquitous Computing; Human-Object Interaction; Large Language Models (LLMs)},
location = {
},
preview={talkingspell.png}
series = {UIST Adjunct '25}
}

@article{wang2025my,
  title={My Dataset of Love': A Preliminary Mixed-Method Exploration of Human-AI Romantic Relationships},
  author={Wang, Xuetong and Pang, Ching Christie and Hui, Pan},
  journal={arXiv preprint arXiv:2508.13655},
  year={2025},
  preview={25-cscw_AI-dating.png}
}

@article{10.1145/3686934,
author = {Wang, Xuetong and Wang, Ziyan and Zhang, Mingmin and Yu, Kangyou and Hui, Pan and Fan, Mingming},
title = {Avatar Appearance and Behavior of Potential Harassers Affect Users' Perceptions and Response Strategies in Social Virtual Reality (VR): A Mixed-Methods Study},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686934},
doi = {10.1145/3686934},
abstract = {Sexual harassment has been recognized as a significant social issue. In recent years, the emergence of harassment in social virtual reality (VR) has become an important and urgent research topic. We employed a mixed-methods approach by conducting online surveys with VR users (N = 166) and semi-structured interviews with social VR users (N = 18) to investigate how users perceive sexual harassment in social VR, focusing on the influence of avatar appearance. Moreover, we derived users' response strategies to sexual harassment and gained insights on platform regulation. This study contributes to the research on sexual harassment in social VR by examining the moderating effect of avatar appearance on user perception of sexual harassment and uncovering the underlying reasons behind response strategies. Moreover, it presents novel prospects and challenges in platform design and regulation domains.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {395},
numpages = {27},
keywords = {avatar appearance, mixed-methods, online sexual harassment, social virtual reality},
preview={CSCW24-harassment-Social-VR.jpg}
}

@inproceedings{10.1145/3678884.3681897,
author = {Yang, Simin and Tsui, Yuk Hang and Wang, Xian and Alhilal, Ahmad and Hadi Mogavi, Reza and Wang, Xuetong and Hui, Pan},
title = {From Prompt to Metaverse: User Perceptions of Personalized Spaces Crafted by Generative AI},
year = {2024},
isbn = {9798400711145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678884.3681897},
doi = {10.1145/3678884.3681897},
abstract = {Generative artificial intelligence (AI) has revolutionized content creation. In parallel, the Metaverse has emerged to transcend the constraints of our physical reality. While Generative AI has a multitude of exciting applications for the fields of writing, coding, and graphic design, its usage to personalize our virtual space has not yet been explored. In this paper, we investigate the application of Artificial Intelligence Generated Content (AIGC) to personalize our virtual spaces and enhance the metaverse experience. To this end, we present a pipeline to enable users to customize their virtual spaces. Moreover, we explore the hardware resources and latency required for personalized spaces, as well as user acceptance of the AI-generated spaces. Comprehensive user studies follow extensive system experiments. Our research evaluates users' perceptions of two generated spaces: panoramic images and 3D virtual spaces. According to our findings, users have shown a great interest in 3D personalized spaces, and the practicality and immersion of 3D space generation tools surpass panoramic space generation tools.},
booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing},
pages = {497–504},
numpages = {8},
keywords = {ai-generated content, generative artificial intelligence, hci, metaverse, personalization, virtual reality, virtual spaces},
location = {San Jose, Costa Rica},
series = {CSCW Companion '24},
}

